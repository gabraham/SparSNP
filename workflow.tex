
\documentclass[a4paper,11pt]{article}

\usepackage{a4wide}
\usepackage[round]{natbib}
\usepackage{graphicx}

\title{SparSNP --- Workflow example}
\author{Gad Abraham gad.abraham@unimelb.edu.au}


\begin{document}

\maketitle

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{process.pdf}
\caption{Analysis workflow}
\label{fig:process}
\end{figure}


Here we describe a typical analysis using SparSNP.  We assume that we have
two datasets in PLINK BED/BIM/FAM format, named \texttt{discovery} and
\texttt{validation}, on the same SNP microarray platform. The phenotype in
the FAM file can be discrete (1 for controls and 2 for cases) or continuous.
In the following, we assume the user is working with a Unix-like command
line shell such as Bash.

\begin{enumerate}

   \item \textbf{Installation}\\

   Requirements:\\
   
      \begin{itemize}
	 \item \textsf{R}, with packages ggplot2 ($\ge 0.9.3$), scales, grid, abind, ROCR
	 \item C compiler, gcc or clang
      \end{itemize}

   Download and install:
\begin{verbatim}
git clone git://github.com/gabraham/SparSNP
cd SparSNP
make
\end{verbatim}

   Prior to running SparSNP you must include the directory in the path:
\begin{verbatim}
export PATH=<PATH_TO_SPARSNP>:$PATH
\end{verbatim}

   

   \item \textbf{Quality control}\\
      SparSNP implements simple imputation of genotypes such that missing
      genotypes are randomly assigned the value \{0,1,2\} with probability
      of~1/3 each, which does not introduce substantial bias to the model when
      the proportion of missingness is small and the genotypes are missing at
      random. We recommend removing samples and SNPs with high missingness,
      and testing for differential missingness between cases and controls.


      \begin{itemize}

      \item Remove markers with missingness $\ge 0.01$, MAF $\le 0.01$, and
      Hardy-Weinberg test for equilibrium in
      controls of $p\le 10^{-4}$, and samples with missingness $\ge 0.01$: \\
      \texttt{\$ plink --bfile discovery --geno 0.01 --mind 0.01
      \textbackslash \\ --maf 0.01 --hwe
      0.0001 \textbackslash \\
      --make-bed --out discovery\_filtered
      }

      \item Test for differential missingness: \\
      \texttt{\$ plink --bfile discovery\_filtered --missing} \\
      (remove SNPs as indicated by output)

      \item Test for sample relatedness with a threshold of $\hat{\pi}=0.05$: \\
      \texttt{\$ plink --bfile discovery\_filtered --Z-genome --min 0.05} \\
      (remove related samples as indicated by output)

      \item Check for stratification using PCA, for example, using
      \texttt{smartpca} in Eigensoft~\citep{price2006}.

      \item Two locus test for detecting batch effects~\citep{lee2010b}. \\
      (remove SNPs as indicated by output)

      \end{itemize}

   \item \textbf{SparSNP on discovery data}


      \begin{itemize}

	 \item Note: your PLINK bed/bim/fam files must have a lower-case suffix
	 (i.e., bed not BED).

	 \item Run cross-validation. By default, $10\times10$-fold
	 cross-validation will be performed, allowing up to $\min \{N, p\}$ SNPs
	 in the model, using a case/control phenotype: \\ for classification
	 (case/control)\\ \texttt{\$ crossval.sh discovery\_filtered sqrhinge}\\
	 and for continuous outputs (linear regression)\\ \texttt{\$ crossval.sh
	 discovery\_filtered linear}\\ The \texttt{crossval.sh} script can be
	 modified to perform more cross-validation folds or to tune the model
	 parameters.

	 You can run SparSNP on 10 cores at once using\\
	 \texttt{\$ NUMPROCS=10 crossval.sh discovery\_filtered sqrhinge}

	 \item Plot AUC and explained phenotypic and genetic variance in the
	 cross-validation replications, with the optional
	 population prevalence $K=1\%$ and heritability $h^2_L=50\%$:\\
	 \texttt{\$ eval.R discovery\_filtered prev=0.01 h2l=0.5} \\
	 The plots \texttt{discovery\_filtered\_AUC.pdf},
	 \texttt{discovery\_filtered\_VarExp.pdf}, and
	 \texttt{discovery\_filtered\_GenVarExp.pdf}
	 will be produced in the
	 directory \texttt{discovery}.
	 The raw AUC and explained variance data is stored in the
	 \texttt{.Rdata} file named \texttt{discovery\_filtered.RData}.
	 The prevalence and heritability are optional. Prevalence is needed
	 for computing explained genetic and phenotypic variance, and heritability is
	 needed for computing explained genetic variance.

	 \item The set of models with best predictive ability will
	 automatically be chosen from the results, based on smoothing
	 of the AUC or $R^2$. The SNPs appearing in these models will be
	 tabulated according to how many time they were included over all
	 cross-validation folds. To inspect the SNPs selected by models that
	 maximise the predictive ability:\\
	 \texttt{head discovery/topsnps.txt}
	 \begin{verbatim}
RS Counts Proportion Replications
rs2050189 60 1 60
rs2187668 60 1 60
rs9357152 60 1 60
rs7774954 60 1 60
rs3129763 58 0.966666666666667 60
...
	 \end{verbatim}
	 The SNPs are ordered by the number of times they were included in a
	 model with non-zero weight (Counts) out of the total number of
	 cross-validation folds (Replications), also shown as a Proportion.
	 SNPs at the top of the list are more stably selected by the lasso and
	 are potentially more robust markers than SNPs at the bottom of the
	 list.

      \end{itemize}

   \item \textbf{Optional: Apply models to validation data}

      SparSNP models trained on the discovery dataset can be tested on an
      independent dataset, if one is available. Datasets can differ in terms
      of which SNPs are assayed, their ordering in the PLINK files, and their
      minor allele frequencies. SparSNP is completely oblivious to these
      differences and cannot detect them, hence we use PLINK scoring. The
      one caveat is that PLINK scoring does not have an intercept term, which
      we must add on. The process is:
      \begin{itemize}

	 \item Extract a consensus model from the training data using
	 \texttt{getmodels.R}, requesting a certain number of SNPs in the
	 model:

	 \texttt{\$ getmodels.R nzreq=256}

	 \item Call \texttt{predict.sh} on the validation dataset which will
	 call PLINK scoring internally:

	 \texttt{\$ predict.sh validation\_filtered}

	 Optionally, you can declare OUTDIR to specify a directory other than
	 ``predict'', and use multi-processing if you have multiple cores:

	 \texttt{\$ NUMPROCS=10 OUTDIR=validation\_filtered\_dir \textbackslash \\
	    predict.sh validation\_filtered}

	 \item Call \texttt{evalprofile.R} to compute ROC/AUC on the
	 validation data, optionally with the prevalence to
	 allow plotting PPV/NPV:

	 \texttt{\$ evalprofile.R model=sqrhinge prev=0.01}

	 If you used an non-default directory for predict, then specify it:

	 \texttt{\$ evalprofile.R model=sqrhinge prev=0.01 outdir=validation\_filtered\_dir}

	 The results will be saved in the directory you specified in the file
	 \texttt{results.RData}.

	 You can now load the results, stored as a list, one element per model
	 in the path, plot the results or compute new one curves using the
	 ROCR \texttt{pred} object:

\begin{verbatim}
> load("predict/results.RData")
> length(res)
[1] 30
> names(res[[10]])
[1] "pred"    "perf"    "sens"    "spec"    "cutoffs" "ppv"     "npv"   
> library(ROCR)
> plot(res[[10]]$perf) # ROC curve for 10th model
> plot(performance(res[[10]]$pred, "prec", "rec")) # precision-recall
\end{verbatim}

      \item \texttt{evalprofile.R} can compute $R^2$ for linear
      regression models, in which case prevalence shouldn't be supplied:

	 \texttt{\$ evalprofile.R model=linear outdir=validation\_filtered\_dir}

	 Note that an alternative phenotype file is currently not supported
	 for \texttt{evalprofile.R}

      \end{itemize}



   \item \textbf{Other post processing}

      \begin{itemize}
	 \item The model weights are stored in each cross-validation
	 directory\\
	 \texttt{discovery/crossvalXX/beta.csv.XX.XX}\\
	 using a sparse text format $<$index,weight$>$,
	 where index is the zero-based index of the SNP in the data (0 is the
	 intercept), and weight is the model weight (a real number).
	 The weights can be read into \textsf{R} 
	 (using \texttt{read.table} with \texttt{sep=",", header=FALSE}) or any other tool for
	 visualisation or for validating the model on other datasets.
      \end{itemize}
      

\end{enumerate}

Some options that can be set by setting the environment variables before
calling \texttt{crossval.sh} are:
\begin{itemize}

   \item \texttt{LAMBDA2}: $\ell_2$  penalty for elastic-net (default=0)

   \item \texttt{NFOLDS}: number of cross-validation folds (default=10)

   \item \texttt{NREPS}: number of cross-validation replications (default=10)

   \item \texttt{NZMAX}: maximum number of SNPs to allow in model
   (default=$\min \{N, p \}$)

   \item \texttt{NLAMBDA1}: number of $\lambda$ penalties on the grid
   (default=30)

   \item \texttt{L1MIN}: multiplier on smallest $\lambda$ used; it should
   be a some positive fraction such as 0.01. Setting it lower will increase
   the number of SNPs in the models, but will also increase computational time
   (default=0.001)

   \item \texttt{PHENO}: for an alternative phenotype file (equivalent to PLINK
   \texttt{--pheno}). The file must be in the format:\\
   FamilyID IndividualID PhenotypeA PhenotypeB ... \\
   \emph{without a header line}.
   When $K$ phenotypes are available SparSNP will fit a multivariate
   model (multi-task), equivalent to $K$ separate models.

\end{itemize}

\bibliography{sparsnp_bmc}
\bibliographystyle{unsrtnat}

\end{document}

